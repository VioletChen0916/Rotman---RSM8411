---
title: "RSM8512 Assignment - NTree and SVM "
author:
  - Yanbing Chen
  - '1009958752'
  - "2023/11/29"
linestretch: 1.25
fontsize: 11pt
geometry: margin=2.5cm
output: 
  pdf_document:
    template: NULL
    latex_engine: xelatex
header-includes:
  - \usepackage{fontspec}
  - \setmainfont{Times New Roman}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(tidyverse)
library(magrittr)
library(ggplot2)
library(tree)
library(randomForest)
```

## Question 1 [20 marks] 

In the lab, a classification tree was applied to the Carseats data set after converting Sales into a qualitative response variable. Now we will seek to predict Sales using regression trees and related approaches, treating the response as quantitative variable.

(a) Split the data set into a training set and a test set.
```{r}
data("Carseats")
set.seed(123)

train = sample(dim(Carseats)[1], dim(Carseats)[1]/2)
Carseats.train = Carseats[train,]
Carseats.test = Carseats[-train,]
```


(b) Fit a regression tree to the training set. Plot the tree, and interpret the results. What test MSE do you obtain?

```{r}
Carseats.tree = tree(Sales ~., data = Carseats.train)
summary(Carseats.tree)
plot(Carseats.tree)
text(Carseats.tree, pretty = 0)
```
```{r}
pred.Carseats = predict(Carseats.tree, Carseats.test)
mean((Carseats.test$Sales - pred.Carseats)^2)
```

The test MSE is 4.395357.

(c) Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?

```{r}
Carseats.cv = cv.tree(Carseats.tree, FUN = prune.tree)
par(mfrow = c(1,2))
plot(Carseats.cv$size, Carseats.cv$dev, type = "b")
plot(Carseats.cv$k, Carseats.cv$dev, type = "b")
```
```{r}
# Best size = 11
Carseats.pruned = prune.tree(Carseats.tree, best = 11)
par(mfrow= c(1,1))
plot(Carseats.pruned)
text(Carseats.pruned, pretty = 0)
```

```{r}
pred.pruned = predict(Carseats.pruned, Carseats.test)
mean((Carseats.test$Sales - pred.pruned)^2)
```

After pruning the tree, the test MSE increase to 4.646.


(d) Use the bagging approach in order to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important.

```{r}
Carseats.bag = randomForest(Sales ~., data = Carseats.train, mtry = 10, ntree = 500, importance = T)
pred.Carseats = predict(Carseats.bag, Carseats.test)
mean((Carseats.test$Sales - pred.Carseats)^2)
```

The test MSE is 2.731395

```{r}
importance(Carseats.bag)
```

Using bagging approach can improve the test MSE to 2.71585. In addition, from te result above we can see `Price`, `ShelveLoc` and `Age` are three most important factors.

(e) Use random forests to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important. Describe the effect of $m$, the number of variables considered at each split, on the error rate obtained.

```{r}
Carseats.rf = randomForest(Sales ~ ., data = Carseats.train, mtry = 5, ntree = 500, 
    importance = T)
pred.rf = predict(Carseats.rf, Carseats.test)
mean((Carseats.test$Sales - pred.rf)^2)
```

The random Forest Model worsen the test MSE to 3.074. Changing $m$ varies test MSE between 2.73 to 3.07.

```{r}
importance(Carseats.rf)
```

From the result above we can see, the three most important factors are `ShelveLoc`, `Price` and `Age`.


## Question 2 [24 marks] 
This problem involves the OJ data set which is part of the ISLR2 package.

(a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.

```{r}
set.seed(1234)
data(OJ)
train = sample(dim(OJ)[1], 800)
OJ_train = OJ[train,]
OJ_test = OJ[-train,]
```

(b) Fit a support vector classifier to the training data using $cost = 0.01$, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics, and describe the results obtained.

```{r}
library(e1071)
svm.linear = svm(Purchase ~ ., kernel = "linear", data = OJ_train, cost = 0.01)
summary(svm.linear)
```

Support Vector classifier creates 437 support vectors out of 800 training points. Out of these, 219 belong to level CH and remaining 218 belong to level MM.

(c) What are the training and test error rates?

```{r}
train.pred = predict(svm.linear, OJ_train)
table(OJ_train$Purchase, train.pred)
(78+57)/(426+57+78+239)
```

The training error rate is 0.16875.

```{r}
test.pred = predict(svm.linear, OJ_test)
table(OJ_test$Purchase, test.pred)
(19+24)/(151+19+24+76)
```

The test error rate is 0.1592593.

(d) Use the tune() function to select an optimal cost. Consider values in the range 0.01 to 10.

```{r}
set.seed(1234)
tune.out = tune(svm, Purchase ~ ., data = OJ_train, kernel = "linear", ranges = list(cost = 10^seq(-2, 
    1, by = 0.25)))
summary(tune.out)
```

The optimal cost is 0.05623.

(e) Compute the training and test error rates using this new value for cost.

```{r}
svm.linear =  svm(Purchase ~ ., kernel = "linear", data = OJ_train, cost = tune.out$best.parameters$cost)
train.pred = predict(svm.linear, OJ_train)
table(OJ_train$Purchase, train.pred)
(57+75)/(426+57+75+242)
```

```{r}
test.pred = predict(svm.linear, OJ_test)
table(OJ_test$Purchase, test.pred)
(18+24)/(152+18+24+76)
```

After using the best cost, the test error rate decreases to 0.1555556, and the training error rate decreases to 0.165.

(f) Repeat parts (b) through (e) using a support vector machine with a radial kernel. Use the default value for gamma.

```{r}
set.seed(1234)
svm.radial = svm(Purchase ~ ., data = OJ_train, kernel = "radial")
summary(svm.radial)
```

```{r}
train.pred = predict(svm.radial, OJ_train)
table(OJ_train$Purchase, train.pred)
(41+79)/(442+238+41+79)
```

```{r}
test.pred = predict(svm.radial, OJ_test)
table(OJ_test$Purchase, test.pred)
(16+26)/(154+16+26+74)
```


```{r}
set.seed(1234)
tune.out = tune(svm, Purchase ~ ., data = OJ_train, kernel = "radial", ranges = list(cost = 10^seq(-2, 
    1, by = 0.25)))
summary(tune.out)
```

The radial basis kernel with default gamma creates 375 support vectors, out of which, 188 belong to level CH
and remaining 187 belong to level MM. The classifier has a training error of 15% and % and a test error of 15.6%. We now use cross validation to find optimal gamma.

```{r}
set.seed(2345)
tune.out = tune(svm, Purchase ~., data = OJ_train, kernel = "radial", ranges = list(cost = 10^seq(-2, 
    1, by = 0.25)))
summary(tune.out)
```

```{r}
svm.radial = svm(Purchase ~ ., data = OJ_train, kernel = "radial", cost = tune.out$best.parameters$cost)
train.pred = predict(svm.radial, OJ_train)
table(OJ_train$Purchase, train.pred)
(48+79)/(435+48+79+239)
```

```{r}
test.pred = predict(svm.radial, OJ_test)
table(OJ_test$Purchase, test.pred)
(17+24)/(153+17+24+76)
```

Tuning does increase the training error but slightly decrease the test error.

(g) Repeat parts (b) through (e) using a support vector machine with a polynomial kernel. Set degree = 2.

```{r}
set.seed(3456)
svm.poly = svm(Purchase ~ ., data = OJ_train, kernel = "poly", degree = 2)
summary(svm.poly)
```

```{r}
train.pred = predict(svm.poly, OJ_train)
table(OJ_train$Purchase, train.pred)
(37+114)/(446+37+114+203)
```

```{r}
test.pred = predict(svm.poly, OJ_test)
table(OJ_test$Purchase, test.pred)
(13+30)/(157+13+30+70)
```

 According to the result above, the polynomial kernel produces 475 support vectors, out of which, 243 belong to level CH and remaining 232 belong to level MM. This kernel produces a train error of 18.9% and a test error of 15.93%, which are higher than the errors produces by radial kernel and the errors produced by linear kernel.
 

```{r}
set.seed(3456)
tune.out = tune(svm, Purchase ~ ., data = OJ_train, kernel = "poly", degree = 2, 
    ranges = list(cost = 10^seq(-2, 1, by = 0.25)))
summary(tune.out)

```

```{r}
svm.poly = svm(Purchase ~ ., data = OJ_train, kernel = "poly", degree = 2, cost = tune.out$best.parameters$cost)
train.pred = predict(svm.poly, OJ_train)
table(OJ_train$Purchase, train.pred)
(40+85)/(443+40+85+232)
```

```{r}
test.pred = predict(svm.poly, OJ_test)
table(OJ_test$Purchase, test.pred)
(15+27)/(155+15+27+73)
```

Tuning reduces the training error to 15.63%, and the test error to 15.56% which is better than before.


(h) Overall, which approach seems to give the best results on this data?

**Answer:** To summarize, radial basis kernel seems to be the best on both train and test data.

## Bonus Question 3 [8 marks]

Here we explore the maximal margin classifier on a toy data set.

(a) We are given $n=7$ observations in $p=2$ dimensions. FOr each observation, there is an associated class label. Sketch the observations.

```{r}
x1 = c(3,2,4,1,2,4,4)
x2 = c(4,2,4,4,1,3,1)
Y = c("red","red","red","red", "blue","blue","blue")
plot(x1,x2, col = Y, xlim = c(0,5), ylim = c(0,5))
```

(b) Sketch the optimal separating hyperplane, and provide the equation for this hyperplane (of the form (9.1)).

**Answer:** The maximal classfier can be calculated by observations #2, #3, #5 and $6.

By transforming their coordinate, we get $(2,1.5), (4, 3.5)$, so we can get $b = (y_2-y_1)/(x_2-x_1) = (3.5-1.5)/(4-2) =1$, $a = x2-x1 = 1.5-2 = -0.5$.
 
```{r}
plot(x1,x2, col = Y, xlim = c(0,5), ylim = c(0,5))
abline(-0.5,1)
```

(c) **Answer:** According to the result gain above,  we know $\beta_0 = 0.5$, $\beta_1=-1$, $\beta_2=1$, So we have $0.5-X_1+X_2 >0$.

(d) On you sketch, indicate the margin for the maximal margin hyperplane.

```{r}
plot(x1, x2, col = Y, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
abline(-1, 1, lty = 2)
abline(0, 1, lty = 2)
```


(e) Indicate the support vectors for the amximal margin classifier.

```{r}
plot(x1, x2, col = Y, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
arrows(2, 1, 2, 1.5)
arrows(2, 2, 2, 1.5)
arrows(4, 4, 4, 3.5)
arrows(4, 3, 4, 3.5)
```


(g) Sketch a hyperplane that is not the optimal separating hyperplane, and provide the equation for this hyperplane.

```{r}
plot(x1, x2, col = Y, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.8,1)
```
The equation is $-0.8-X_1+X_2>0$.

(h) Draw an additional observation on the plot so that the two classes are no longer separable by a hyperplane.

```{r}
plot(x1, x2, col = Y, xlim = c(0, 5), ylim = c(0, 5))
points(c(4), c(2), col = c("red"))
```



